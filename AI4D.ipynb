{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "AI4D.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cTYpW8ZRE5G"
      },
      "source": [
        "### Author - **Nilesh Bansal**\r\n",
        "\r\n",
        "### Objective - NMT using Attention to translate French into Fongbe and Ewe (two African languages - collectively called Gbe)\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9I6hAntMSArw"
      },
      "source": [
        "#### Step 1 - install relevent libraries and load them"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G2duXelWRd7G",
        "outputId": "a80f0680-8fc9-4c20-86bd-987e83dd10bd"
      },
      "source": [
        "!pip -q install trax\r\n",
        "!pip install nltk"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 522kB 13.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 235kB 30.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.4MB 31.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.2MB 56.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 368kB 58.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 61kB 7.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.8MB 54.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.9MB 53.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.2MB 52.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 890kB 60.4MB/s \n",
            "\u001b[?25h  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cVHvQ66FWNDm"
      },
      "source": [
        "from termcolor import colored\r\n",
        "import random\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "\r\n",
        "import trax\r\n",
        "from trax import layers as tl\r\n",
        "from trax.fastmath import numpy as fastnp\r\n",
        "from trax.supervised import training\r\n",
        "from trax.supervised import decoding\r\n",
        "import textwrap\r\n",
        "\r\n",
        "import pickle\r\n",
        "import string\r\n",
        "import ast\r\n",
        "\r\n",
        "import nltk\r\n",
        "from nltk.tokenize import TweetTokenizer\r\n",
        "\r\n",
        "wrapper = textwrap.TextWrapper(width=70)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ITWkh2p3Wi8X",
        "outputId": "99b02f0c-092e-487f-9382-7abbd7848da6"
      },
      "source": [
        "!pip list | grep trax"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "trax                          1.3.7                \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6TMFIY2CTCSU"
      },
      "source": [
        "#### Step 2 - Load the train, test sets from the google drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1KIAS0iXBgC"
      },
      "source": [
        "df_train = pd.read_csv(r\"/content/drive/MyDrive/competitions/Zindi Competition_NMT_AI4D/Train.csv\")\r\n",
        "df_test = pd.read_csv(r\"/content/drive/MyDrive/competitions/Zindi Competition_NMT_AI4D/Test.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRcDrQfFMnEK"
      },
      "source": [
        "#### Step 3 - separate the data to convert \r\n",
        "1. french to fongbe \\\r\n",
        "2. french to ewe \\\r\n",
        "may be use the trained model from 1st and use transfer learning on 2nd"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGjZLmZHMlHX"
      },
      "source": [
        "# prepare train and eval sets\r\n",
        "\r\n",
        "df_train_fr_to_fo = df_train[df_train[\"Target_Language\"] == \"Fon\"]\r\n",
        "df_train_fr_to_ewe = df_train[df_train[\"Target_Language\"] == \"Ewe\"]\r\n",
        "\r\n",
        "data_x_fr_to_fo = list(df_train_fr_to_fo[\"French\"])\r\n",
        "data_y_fr_to_fo = list(df_train_fr_to_fo[\"Target\"])\r\n",
        "\r\n",
        "# define train (97% of data) and eval (3% of data)\r\n",
        "div_slice_ff = len(data_x_fr_to_fo) - int(len(data_x_fr_to_fo) * 0.03)\r\n",
        "data_x_fr_to_fo_train = data_x_fr_to_fo[:div_slice_ff]\r\n",
        "data_x_fr_to_fo_eval = data_x_fr_to_fo[div_slice_ff:]\r\n",
        "data_y_fr_to_fo_train = data_y_fr_to_fo[:div_slice_ff]\r\n",
        "data_y_fr_to_fo_eval = data_y_fr_to_fo[div_slice_ff:]\r\n",
        "assert(len(data_x_fr_to_fo_train) + len(data_x_fr_to_fo_eval) == len(data_x_fr_to_fo))\r\n",
        "\r\n",
        "data_x_fr_to_ewe = list(df_train_fr_to_ewe[\"French\"])\r\n",
        "data_y_fr_to_ewe = list(df_train_fr_to_ewe[\"Target\"])\r\n",
        "\r\n",
        "div_slice_fe = len(data_x_fr_to_ewe) - int(len(data_x_fr_to_ewe) * 0.03)\r\n",
        "data_x_fr_to_ewe_train = data_x_fr_to_ewe[:div_slice_fe]\r\n",
        "data_x_fr_to_ewe_eval = data_x_fr_to_ewe[div_slice_fe:]\r\n",
        "data_y_fr_to_ewe_train = data_y_fr_to_ewe[:div_slice_fe]\r\n",
        "data_y_fr_to_ewe_eval = data_y_fr_to_ewe[div_slice_fe:]\r\n",
        "assert(len(data_x_fr_to_ewe_train) + len(data_x_fr_to_ewe_eval) == len(data_x_fr_to_ewe))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3mRRYiSaAUo"
      },
      "source": [
        "#### Create vocab files for all 3 languages\r\n",
        "1. French\r\n",
        "2. Fongbe\r\n",
        "3. Ewe \\\r\n",
        "\r\n",
        "Later work on BPE for each of the languages for better results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJc4HpMugJc3"
      },
      "source": [
        "def create_vocab(language_text):\r\n",
        "  vocab_list = []\r\n",
        "  for elem in language_text:\r\n",
        "    tokenizer = TweetTokenizer()\r\n",
        "    tokenized_elem = tokenizer.tokenize(elem)\r\n",
        "    vocab_list.extend(tokenized_elem)\r\n",
        "  \r\n",
        "  # add <pad> and <EOS> to the vocab, no need to add <UNK>\r\n",
        "  vocab_list = list(set(vocab_list))\r\n",
        "  return vocab_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FoFgWgH_h92t"
      },
      "source": [
        "french_vocab_list = create_vocab(data_x_fr_to_fo + data_x_fr_to_ewe)\r\n",
        "fongbe_vocab_list = create_vocab(data_y_fr_to_fo)\r\n",
        "ewe_vocab_list = create_vocab(data_y_fr_to_ewe)\r\n",
        "vocab_list = [\"<pad>\", \"<EOS>\"] + french_vocab_list + fongbe_vocab_list + ewe_vocab_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zp5ldST8hFGF"
      },
      "source": [
        "with open(r\"/content/drive/MyDrive/competitions/Zindi Competition_NMT_AI4D/fr_vocab.txt\", \"w\") as output:\r\n",
        "  for elem in vocab_list:\r\n",
        "    output.write(elem + \"_\" + \"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HclTiefvYAOS"
      },
      "source": [
        "#### Step 4 - Develop data generator\r\n",
        "to yield (x, y) tuples, one pair at a time"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hcqT91ZGYFNY"
      },
      "source": [
        "def data_generator(data_x, data_y, shuffle = False):\r\n",
        "\r\n",
        "  # len of data_x and data_y must be same  \r\n",
        "  assert(len(data_x) == len(data_y))\r\n",
        "  assert(len(data_x) != 0)\r\n",
        "\r\n",
        "  data_lng = len(data_x)\r\n",
        "  index_list = [*range(data_lng)]\r\n",
        "\r\n",
        "  # Shuffle the data if set to True\r\n",
        "  if shuffle:\r\n",
        "    random.shuffle(index_list)\r\n",
        "\r\n",
        "  while True:\r\n",
        "\r\n",
        "    if shuffle:\r\n",
        "      random.shuffle(index_list)\r\n",
        "\r\n",
        "    X = data_x[index_list[0]]\r\n",
        "    Y = data_y[index_list[0]]\r\n",
        "    \r\n",
        "    yield ((X, Y))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RmMbKbeqkOQc"
      },
      "source": [
        "# french to ewe\r\n",
        "train_stream_fe = data_generator(data_x_fr_to_ewe_train, data_y_fr_to_ewe_train, shuffle = True)\r\n",
        "eval_stream_fe = data_generator(data_x_fr_to_ewe_eval, data_y_fr_to_ewe_eval, shuffle = True)\r\n",
        "\r\n",
        "# french to fongbe\r\n",
        "train_stream_ff = data_generator(data_x_fr_to_fo_train, data_y_fr_to_fo_train, shuffle = True)\r\n",
        "eval_stream_ff = data_generator(data_x_fr_to_fo_eval, data_y_fr_to_fo_eval, shuffle = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVRaP-P8jh5b"
      },
      "source": [
        "#### Step 5 - Tokenize and format"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_5LUBoi8QnZ4"
      },
      "source": [
        "# global variables that state the filename and directory of the vocabulary file\r\n",
        "VOCAB_FILE = \"vocab.txt\"\r\n",
        "VOCAB_DIR = \"/content/drive/MyDrive/competitions/Zindi Competition_NMT_AI4D\"\r\n",
        "\r\n",
        "# french to ewe tokenization\r\n",
        "tokenized_train_stream_fe = trax.data.Tokenize(vocab_type = 'char', vocab_file = VOCAB_FILE, vocab_dir=VOCAB_DIR)(train_stream_fe)\r\n",
        "tokenized_eval_stream_fe = trax.data.Tokenize(vocab_type = 'char', vocab_file = VOCAB_FILE, vocab_dir=VOCAB_DIR)(eval_stream_fe)\r\n",
        "\r\n",
        "# french to fongbe tokenization\r\n",
        "tokenized_train_stream_ff = trax.data.Tokenize(vocab_type = 'char', vocab_file = VOCAB_FILE, vocab_dir=VOCAB_DIR)(train_stream_ff)\r\n",
        "tokenized_eval_stream_ff = trax.data.Tokenize(vocab_type = 'char', vocab_file = VOCAB_FILE, vocab_dir=VOCAB_DIR)(eval_stream_ff)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ASwdFvA48jSq"
      },
      "source": [
        "# global variable to used in the NMT model\r\n",
        "input_vocab_size = trax.data.vocab_size(\r\n",
        "    vocab_type='char',\r\n",
        "    vocab_file=VOCAB_FILE,\r\n",
        "    vocab_dir=VOCAB_DIR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4WAiRnjskiFI"
      },
      "source": [
        "# Append EOS at the end of each sentence\r\n",
        "# Assign <EOS> = 1\r\n",
        "EOS = 1\r\n",
        "\r\n",
        "# Generator helper function to append <EOS> to each sentence\r\n",
        "def append_eos(stream):\r\n",
        "  for (input, target) in stream:\r\n",
        "    input_with_eos = list(input) + [EOS]\r\n",
        "    target_with_eos = list(target) + [EOS]\r\n",
        "    yield np.array(input_with_eos), np.array(target_with_eos)\r\n",
        "\r\n",
        "# french to ewe\r\n",
        "# append EOS to the train data\r\n",
        "tokenized_train_stream_fe = append_eos(tokenized_train_stream_fe)\r\n",
        "# append EOS to the eval data\r\n",
        "tokenized_eval_stream_fe = append_eos(tokenized_eval_stream_fe)\r\n",
        "\r\n",
        "# french to fongbe\r\n",
        "# append EOS to the train data\r\n",
        "tokenized_train_stream_ff = append_eos(tokenized_train_stream_ff)\r\n",
        "# append EOS to the eval data\r\n",
        "tokenized_eval_stream_ff = append_eos(tokenized_eval_stream_ff)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cMvEbSxXmRI-"
      },
      "source": [
        "# filter bymax length - Later - the data is already processed in this case\r\n",
        "# pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9eBN6Q7mgu6"
      },
      "source": [
        "# tokenize and detokenize helper functions\r\n",
        "def tokenize(input_str, vocab_file = None, vocab_dir = None):\r\n",
        "  \"\"\" Encodes a string to array of numbers\r\n",
        "\r\n",
        "  Args:\r\n",
        "      input_str (str): human readable string to encode\r\n",
        "      vocab_file (list): list of all words in the vocab French, Fongbe, Ewe combined\r\n",
        "\r\n",
        "  Returns:\r\n",
        "      numpy.ndarray: tokenized version of input string\r\n",
        "  \"\"\"\r\n",
        "  # set encoding of EOS as 1\r\n",
        "  EOS = 1\r\n",
        "\r\n",
        "  inputs = next(trax.data.tokenize(iter([input_str]), vocab_type = 'char',\r\n",
        "                                   vocab_file=vocab_file, vocab_dir=vocab_dir))\r\n",
        "  inputs = list(inputs) + [EOS]\r\n",
        "\r\n",
        "  # Adding batch dimension to the front of shape\r\n",
        "  batch_inputs = np.reshape(np.array(inputs), [1, -1])\r\n",
        "\r\n",
        "  return batch_inputs\r\n",
        "\r\n",
        "def detokenize(integers, vocab_file = None, vocab_dir = None):\r\n",
        "  \"\"\" Decodes an array of integers to human readable string\r\n",
        "\r\n",
        "  Args:\r\n",
        "      integers (numpy.ndarray): array of integers to decode\r\n",
        "      vocab_file (list): list of all words in the vocab French, Fongbe, Ewe combined\r\n",
        "\r\n",
        "  Returns:\r\n",
        "      str: decoded sentence\r\n",
        "  \"\"\"\r\n",
        "\r\n",
        "  # remove dimension of size 1\r\n",
        "  integers = list(np.squeeze(integers))\r\n",
        "\r\n",
        "  EOS = 1\r\n",
        "\r\n",
        "  # remove EOS \r\n",
        "  if EOS in integers:\r\n",
        "    integers = integers[:integers.index(EOS)]\r\n",
        "\r\n",
        "  return trax.data.detokenize(integers, vocab_type = 'char',\r\n",
        "                              vocab_file=vocab_file, vocab_dir=vocab_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5isXBH0Uml57",
        "outputId": "439608b5-1564-4a5a-d523-89c9c4aab4f1"
      },
      "source": [
        "# check for tokenize and detokenize functionality\r\n",
        "train_input, train_target = next(tokenized_train_stream_fe)\r\n",
        "# Detokenize an input-target pair of tokenized sentences\r\n",
        "print(colored(f'Single detokenized example input:', 'red'), detokenize(train_input, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR))\r\n",
        "print(colored(f'Single detokenized example target:', 'red'), detokenize(train_target, vocab_file=\"fo_vocab.txt\", vocab_dir=VOCAB_DIR))\r\n",
        "print()\r\n",
        "\r\n",
        "# Tokenize and detokenize a word that is not explicitly saved in the vocabulary file.\r\n",
        "# This is not applicable rightnow since SPE not created, just used for checking the function\r\n",
        "print(colored(f\"tokenize('baptise'): \", 'green'), tokenize('baptise', vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR))\r\n",
        "print(colored(f\"detokenize(train_input): \", 'green'), detokenize(train_input, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR))\r\n",
        "print(colored(f\"detokenize(train_target): \", 'green'), detokenize(train_target, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[31mSingle detokenized example input:\u001b[0m  Je t’offre un toit ce soir, et comme on ne sait jamais… Bref, c’est toi qui vois, Jessica\n",
            "\u001b[31mSingle detokenized example target:\u001b[0m mana mlɔƒe wo zã sia me eye esi ame aɖe menya nui o la, wo ŋutɔ kpɔe ɖae Jessica\n",
            "\n",
            "\u001b[32mtokenize('baptise'): \u001b[0m [[ 98  97 112 116 105 115 101   1]]\n",
            "\u001b[32mdetokenize(train_input): \u001b[0m  Je t’offre un toit ce soir, et comme on ne sait jamais… Bref, c’est toi qui vois, Jessica\n",
            "\u001b[32mdetokenize(train_target): \u001b[0m mana mlɔƒe wo zã sia me eye esi ame aɖe menya nui o la, wo ŋutɔ kpɔe ɖae Jessica\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4l1NNvFNUMmJ"
      },
      "source": [
        "#### Step 6 - Bucketing for creating streams of batches"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KY9pBJvUw3ZT"
      },
      "source": [
        "# Buckets are defined in terms of boundaries and batch sizes.\r\n",
        "# Batch_sizes[i] determines the batch size for items with length < boundaries[i]\r\n",
        "# So below, we'll take a batch of 256 sentences of length < 8, 128 if length is\r\n",
        "# between 8 and 16, and so on -- and only 2 if length is over 512.\r\n",
        "boundaries =  [8,   16,  32, 64, 128, 256, 512]\r\n",
        "batch_sizes = [256, 128, 64, 32, 16,    8,   4,  2]\r\n",
        "\r\n",
        "# create the generators french to ewe\r\n",
        "train_batch_stream_fe = trax.data.BucketByLength(\r\n",
        "    boundaries, batch_sizes,\r\n",
        "    length_keys=[0, 1]  # As before: count inputs and targets to length.\r\n",
        ")(tokenized_train_stream_fe)\r\n",
        "eval_batch_stream_fe = trax.data.BucketByLength(\r\n",
        "    boundaries, batch_sizes,\r\n",
        "    length_keys=[0, 1]  # As before: count inputs and targets to length.\r\n",
        ")(tokenized_eval_stream_fe)\r\n",
        "\r\n",
        "# create the generators french to ewe\r\n",
        "train_batch_stream_ff = trax.data.BucketByLength(\r\n",
        "    boundaries, batch_sizes,\r\n",
        "    length_keys=[0, 1]  # As before: count inputs and targets to length.\r\n",
        ")(tokenized_train_stream_ff)\r\n",
        "eval_batch_stream_ff = trax.data.BucketByLength(\r\n",
        "    boundaries, batch_sizes,\r\n",
        "    length_keys=[0, 1]  # As before: count inputs and targets to length.\r\n",
        ")(tokenized_eval_stream_ff)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5DSgkrQgxxo4"
      },
      "source": [
        "# Add masking for the padding (0s) - french to ewe\r\n",
        "train_batch_stream_fe = trax.data.AddLossWeights(id_to_mask=0)(train_batch_stream_fe)\r\n",
        "eval_batch_stream_fe = trax.data.AddLossWeights(id_to_mask=0)(eval_batch_stream_fe)\r\n",
        "\r\n",
        "# Add masking for the padding (0s) - french to fongbe\r\n",
        "train_batch_stream_ff = trax.data.AddLossWeights(id_to_mask=0)(train_batch_stream_ff)\r\n",
        "eval_batch_stream_ff = trax.data.AddLossWeights(id_to_mask=0)(eval_batch_stream_ff)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mg9lpo3aGBax"
      },
      "source": [
        "#### Helper functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3u52350xGGEN"
      },
      "source": [
        "##### Input encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-c8wkyxUGE8N"
      },
      "source": [
        "# inspired from NLP specialization\r\n",
        "def input_encoder_fn(input_vocab_size, d_model, n_encoder_layers):\r\n",
        "    \"\"\" Input encoder runs on the input sentence and creates\r\n",
        "    activations that will be the keys and values for attention.\r\n",
        "    \r\n",
        "    Args:\r\n",
        "        input_vocab_size: int: vocab size of the input\r\n",
        "        d_model: int:  depth of embedding (n_units in the LSTM cell)\r\n",
        "        n_encoder_layers: int: number of LSTM layers in the encoder\r\n",
        "    Returns:\r\n",
        "        tl.Serial: The input encoder\r\n",
        "    \"\"\"\r\n",
        "    \r\n",
        "    # create a serial network\r\n",
        "    input_encoder = tl.Serial( \r\n",
        "        \r\n",
        "        # create an embedding layer to convert tokens to vectors\r\n",
        "        tl.Embedding(input_vocab_size, d_model),\r\n",
        "        \r\n",
        "        # feed the embeddings to the LSTM layers. It is a stack of n_encoder_layers LSTM layers\r\n",
        "        [tl.LSTM(d_model) for x in range(n_encoder_layers)]\r\n",
        "    )\r\n",
        "    \r\n",
        "    return input_encoder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ioFy0SjpGuf6"
      },
      "source": [
        "##### Pre attention decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adMYENNNGz6F"
      },
      "source": [
        "# inspired from NLP specialization\r\n",
        "def pre_attention_decoder_fn(mode, target_vocab_size, d_model):\r\n",
        "    \"\"\" Pre-attention decoder runs on the targets and creates\r\n",
        "    activations that are used as queries in attention.\r\n",
        "    \r\n",
        "    Args:\r\n",
        "        mode: str: 'train' or 'eval'\r\n",
        "        target_vocab_size: int: vocab size of the target\r\n",
        "        d_model: int:  depth of embedding (n_units in the LSTM cell)\r\n",
        "    Returns:\r\n",
        "        tl.Serial: The pre-attention decoder\r\n",
        "    \"\"\"\r\n",
        "    \r\n",
        "    # create a serial network\r\n",
        "    pre_attention_decoder = tl.Serial(\r\n",
        "        \r\n",
        "        # shift right to insert start-of-sentence token and implement\r\n",
        "        # teacher forcing during training\r\n",
        "        tl.ShiftRight(),\r\n",
        "\r\n",
        "        # run an embedding layer to convert tokens to vectors\r\n",
        "        tl.Embedding(target_vocab_size, d_model),\r\n",
        "\r\n",
        "        # feed to an LSTM layer\r\n",
        "        tl.LSTM(d_model)\r\n",
        "    )\r\n",
        "    \r\n",
        "    return pre_attention_decoder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKEczsd_HO3B"
      },
      "source": [
        "##### Prepare attention input"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7j7MIuvHHTNV"
      },
      "source": [
        "# inspired from NLP specialization\r\n",
        "def prepare_attention_input(encoder_activations, decoder_activations, inputs):\r\n",
        "    \"\"\"Prepare queries, keys, values and mask for attention.\r\n",
        "    \r\n",
        "    Args:\r\n",
        "        encoder_activations fastnp.array(batch_size, padded_input_length, d_model): output from the input encoder\r\n",
        "        decoder_activations fastnp.array(batch_size, padded_input_length, d_model): output from the pre-attention decoder\r\n",
        "        inputs fastnp.array(batch_size, padded_input_length): padded input tokens\r\n",
        "    \r\n",
        "    Returns:\r\n",
        "        queries, keys, values and mask for attention.\r\n",
        "    \"\"\"\r\n",
        "    \r\n",
        "    # set the keys and values to the encoder activations\r\n",
        "    keys = encoder_activations\r\n",
        "    values = encoder_activations\r\n",
        "\r\n",
        "    \r\n",
        "    # set the queries to the decoder activations\r\n",
        "    queries = decoder_activations\r\n",
        "    \r\n",
        "    # generate the mask to distinguish real tokens from padding\r\n",
        "    # hint: inputs is 1 for real tokens and 0 where they are padding\r\n",
        "    print(inputs)\r\n",
        "    mask = fastnp.where(inputs > 0, 1, inputs)\r\n",
        "    \r\n",
        "    # add axes to the mask for attention heads and decoder length.\r\n",
        "    mask = fastnp.reshape(mask, (mask.shape[0], 1, 1, mask.shape[1]))\r\n",
        "    \r\n",
        "    # broadcast so mask shape is [batch size, attention heads, decoder-len, encoder-len].\r\n",
        "    # note: for this assignment, attention heads is set to 1.\r\n",
        "    mask = mask + fastnp.zeros((1, 1, decoder_activations.shape[1], 1))\r\n",
        "    \r\n",
        "    return queries, keys, values, mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vdvtpn78Il2H"
      },
      "source": [
        "#### Step 7 - NMT Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rt6bW7zfIrpk"
      },
      "source": [
        "def NMTAttn(input_vocab_size=input_vocab_size,\r\n",
        "            target_vocab_size=input_vocab_size,\r\n",
        "            d_model=1024,\r\n",
        "            n_encoder_layers=24,\r\n",
        "            n_decoder_layers=24,\r\n",
        "            n_attention_heads=16,\r\n",
        "            attention_dropout=0.1,\r\n",
        "            mode='train'):\r\n",
        "    \"\"\"Returns an LSTM sequence-to-sequence model with attention.\r\n",
        "\r\n",
        "    The input to the model is a pair (input tokens, target tokens), e.g.,\r\n",
        "    an English sentence (tokenized) and its translation into German (tokenized).\r\n",
        "\r\n",
        "    Args:\r\n",
        "    input_vocab_size: int: vocab size of the input\r\n",
        "    target_vocab_size: int: vocab size of the target\r\n",
        "    d_model: int:  depth of embedding (n_units in the LSTM cell)\r\n",
        "    n_encoder_layers: int: number of LSTM layers in the encoder\r\n",
        "    n_decoder_layers: int: number of LSTM layers in the decoder after attention\r\n",
        "    n_attention_heads: int: number of attention heads\r\n",
        "    attention_dropout: float, dropout for the attention layer\r\n",
        "    mode: str: 'train', 'eval' or 'predict', predict mode is for fast inference\r\n",
        "\r\n",
        "    Returns:\r\n",
        "    A LSTM sequence-to-sequence model with attention.\r\n",
        "    \"\"\"\r\n",
        "    \r\n",
        "    # Step 0: call the helper function to create layers for the input encoder\r\n",
        "    input_encoder = input_encoder_fn(input_vocab_size, d_model, n_encoder_layers)\r\n",
        "\r\n",
        "    # Step 0: call the helper function to create layers for the pre-attention decoder\r\n",
        "    pre_attention_decoder = pre_attention_decoder_fn(mode, target_vocab_size, d_model)\r\n",
        "\r\n",
        "    # Step 1: create a serial network\r\n",
        "    model = tl.Serial( \r\n",
        "        \r\n",
        "      # Step 2: copy input tokens and target tokens as they will be needed later.\r\n",
        "      tl.Select([0, 1, 0, 1]),\r\n",
        "        \r\n",
        "      # Step 3: run input encoder on the input and pre-attention decoder the target.\r\n",
        "      tl.Parallel(input_encoder, pre_attention_decoder),\r\n",
        "        \r\n",
        "      # Step 4: prepare queries, keys, values and mask for attention.\r\n",
        "      tl.Fn('PrepareAttentionInput', prepare_attention_input, n_out=4),\r\n",
        "        \r\n",
        "      # Step 5: run the AttentionQKV layer\r\n",
        "      # nest it inside a Residual layer to add to the pre-attention decoder activations(i.e. queries)\r\n",
        "      tl.Residual(tl.AttentionQKV(d_model, n_heads=n_attention_heads, dropout=attention_dropout, mode=mode)),\r\n",
        "      \r\n",
        "      # Step 6: drop attention mask (i.e. index = None\r\n",
        "      tl.Select([0, 2]),\r\n",
        "        \r\n",
        "      # Step 7: run the rest of the RNN decoder\r\n",
        "      [tl.LSTM(d_model) for _ in range(n_decoder_layers)],\r\n",
        "        \r\n",
        "      # Step 8: prepare output by making it the right size\r\n",
        "      tl.Dense(target_vocab_size),\r\n",
        "        \r\n",
        "      # Step 9: Log-softmax for output\r\n",
        "      tl.LogSoftmax()\r\n",
        "    )\r\n",
        "    \r\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zrp56zvyJYnB"
      },
      "source": [
        "#### Step 8 - Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AW8O-By0Jg03"
      },
      "source": [
        "##### Train Task"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6vShVBKYI78m"
      },
      "source": [
        "# train task - French to Ewe\r\n",
        "train_task_fe = training.TrainTask(\r\n",
        "\r\n",
        "    # use the train batch stream as labeled data\r\n",
        "    labeled_data= train_batch_stream_fe,\r\n",
        "    \r\n",
        "    # use the cross entropy loss\r\n",
        "    loss_layer= tl.CrossEntropyLoss(),\r\n",
        "    \r\n",
        "    # use the Adam optimizer with learning rate of 0.01\r\n",
        "    optimizer= trax.optimizers.Adam(0.01),\r\n",
        "    \r\n",
        "    # use the `trax.lr.warmup_and_rsqrt_decay` as the learning rate schedule\r\n",
        "    # have 1000 warmup steps with a max value of 0.01\r\n",
        "    lr_schedule= trax.lr.warmup_and_rsqrt_decay(1000, 0.01),\r\n",
        "    \r\n",
        "    # have a checkpoint every 10 steps\r\n",
        "    n_steps_per_checkpoint= 10,\r\n",
        "    \r\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBqLPNALJjFf"
      },
      "source": [
        "##### Eval task"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KaWDhEgiJbAm"
      },
      "source": [
        "eval_task_fe = training.EvalTask(\r\n",
        "    \r\n",
        "    ## use the eval batch stream as labeled data\r\n",
        "    labeled_data=eval_batch_stream_fe,\r\n",
        "    \r\n",
        "    ## use the cross entropy loss and accuracy as metrics\r\n",
        "    metrics=[tl.CrossEntropyLoss(), tl.Accuracy()],\r\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KTJbDmxBcINf"
      },
      "source": [
        "##### Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wbBra8wBJp81",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50a515a7-1699-4a1a-a56b-3f77cde22abb"
      },
      "source": [
        "# define the output directory\r\n",
        "output_dir = r\"/content/drive/MyDrive/competitions/Zindi Competition_NMT_AI4D/output\"\r\n",
        "\r\n",
        "# remove old model if it exists. restarts training.\r\n",
        "!rm -f ~/output_dir/model.pkl.gz  \r\n",
        "\r\n",
        "# define the training loop\r\n",
        "training_loop = training.Loop(NMTAttn(mode='train'),\r\n",
        "                              train_task_fe,\r\n",
        "                              eval_tasks=[eval_task_fe],\r\n",
        "                              output_dir=output_dir)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Traced<ShapedArray(int32[16,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "Traced<ShapedArray(int32[16,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "Traced<ShapedArray(int32[8,256])>with<DynamicJaxprTrace(level=1/0)>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9P8yyCScc4n"
      },
      "source": [
        "# NOTE: Execute the training loop.\r\n",
        "# Output model saved in google drive\r\n",
        "training_loop.run(200)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KOz7YjCccuaX",
        "outputId": "0cf49ebb-293d-45d6-8550-4d4f6b597d63"
      },
      "source": [
        "# instantiate the model we built in eval mode\r\n",
        "model = NMTAttn(mode='predict')\r\n",
        "\r\n",
        "# initialize weights from a pre-trained model\r\n",
        "model.init_from_file(r\"/content/drive/MyDrive/competitions/Zindi Competition_NMT_AI4D/output/model.pkl.gz\", weights_only=True)\r\n",
        "model = tl.Accelerate(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Traced<ShapedArray(int32[16,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "Traced<ShapedArray(int32[16,128])>with<DynamicJaxprTrace(level=1/0)>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bczUdRYwaVlZ"
      },
      "source": [
        "# inspired from NLP specialization\r\n",
        "def next_symbol(NMTAttn, input_tokens, cur_output_tokens, temperature):\r\n",
        "    \"\"\"Returns the index of the next token.\r\n",
        "\r\n",
        "    Args:\r\n",
        "        NMTAttn (tl.Serial): An LSTM sequence-to-sequence model with attention.\r\n",
        "        input_tokens (np.ndarray 1 x n_tokens): tokenized representation of the input sentence\r\n",
        "        cur_output_tokens (list): tokenized representation of previously translated words\r\n",
        "        temperature (float): parameter for sampling ranging from 0.0 to 1.0.\r\n",
        "            0.0: same as argmax, always pick the most probable token\r\n",
        "            1.0: sampling from the distribution (can sometimes say random things)\r\n",
        "\r\n",
        "    Returns:\r\n",
        "        int: index of the next token in the translated sentence\r\n",
        "        float: log probability of the next symbol\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    # set the length of the current output tokens\r\n",
        "    token_length = len(cur_output_tokens)\r\n",
        "\r\n",
        "    # calculate next power of 2 for padding length \r\n",
        "    padded_length = 2**int(np.ceil(np.log2(token_length + 1)))\r\n",
        "\r\n",
        "    # pad cur_output_tokens up to the padded_length\r\n",
        "    padded = cur_output_tokens + [0 for _ in range(padded_length - token_length)]\r\n",
        "    \r\n",
        "    # model expects the output to have an axis for the batch size in front so\r\n",
        "    # convert `padded` list to a numpy array with shape (x, <padded_length>) where the\r\n",
        "    # x position is the batch axis. (hint: you can use np.expand_dims() with axis=0 to insert a new axis)\r\n",
        "    padded_with_batch = np.expand_dims(padded, axis = 0)\r\n",
        "\r\n",
        "    # get the model prediction. remember to use the `NMTAttn` argument defined above.\r\n",
        "    # hint: the model accepts a tuple as input (e.g. `my_model((input1, input2))`)\r\n",
        "    output, _ = NMTAttn((input_tokens, padded_with_batch))\r\n",
        "    \r\n",
        "    # get log probabilities from the last token output\r\n",
        "    log_probs = output[0, len(cur_output_tokens), :]\r\n",
        "\r\n",
        "    # get the next symbol by getting a logsoftmax sample (*hint: cast to an int)\r\n",
        "    symbol = np.int(tl.logsoftmax_sample(log_probs,temperature=temperature))\r\n",
        "\r\n",
        "    return symbol, float(log_probs[symbol])\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d0biKtMXae94"
      },
      "source": [
        "def sampling_decode(input_sentence, NMTAttn = None, temperature=0.0, vocab_file=None, vocab_dir=None):\r\n",
        "    \"\"\"Returns the translated sentence.\r\n",
        "\r\n",
        "    Args:\r\n",
        "        input_sentence (str): sentence to translate.\r\n",
        "        NMTAttn (tl.Serial): An LSTM sequence-to-sequence model with attention.\r\n",
        "        temperature (float): parameter for sampling ranging from 0.0 to 1.0.\r\n",
        "            0.0: same as argmax, always pick the most probable token\r\n",
        "            1.0: sampling from the distribution (can sometimes say random things)\r\n",
        "        vocab_file (str): filename of the vocabulary\r\n",
        "        vocab_dir (str): path to the vocabulary file\r\n",
        "\r\n",
        "    Returns:\r\n",
        "        tuple: (list, str, float)\r\n",
        "            list of int: tokenized version of the translated sentence\r\n",
        "            float: log probability of the translated sentence\r\n",
        "            str: the translated sentence\r\n",
        "    \"\"\"\r\n",
        "    \r\n",
        "    # encode the input sentence\r\n",
        "    input_tokens = tokenize(input_sentence, vocab_file, vocab_dir)\r\n",
        "    \r\n",
        "    # initialize the list of output tokens\r\n",
        "    cur_output_tokens = []\r\n",
        "    \r\n",
        "    # initialize an integer that represents the current output index\r\n",
        "    cur_output = 0\r\n",
        "    \r\n",
        "    # Set the encoding of the \"end of sentence\" as 1\r\n",
        "    EOS = 1\r\n",
        "    \r\n",
        "    # check that the current output is not the end of sentence token\r\n",
        "    while cur_output != EOS:\r\n",
        "        \r\n",
        "        # update the current output token by getting the index of the next word (hint: use next_symbol)\r\n",
        "        cur_output, log_prob = next_symbol(NMTAttn, input_tokens, cur_output_tokens, temperature)\r\n",
        "        \r\n",
        "        # append the current output token to the list of output tokens\r\n",
        "        cur_output_tokens.append(cur_output)\r\n",
        "    \r\n",
        "    # detokenize the output tokens\r\n",
        "    sentence = detokenize(cur_output_tokens, vocab_file, vocab_dir)\r\n",
        "    \r\n",
        "    return cur_output_tokens, log_prob, sentence\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EPolhgh7ag4Z"
      },
      "source": [
        "# Test the function above.\r\n",
        "# Change temperature variable and see variations in results\r\n",
        "# Run it several times with each setting and see how often the output changes.\r\n",
        "sampling_decode(\"Oui, parce que même dans le drame, le Camerounais aime le sensationnel. \", model, temperature=0.0, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}